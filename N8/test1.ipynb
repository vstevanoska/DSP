{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base\n",
    "import openl3\n",
    "import librosa # alternativa pyAudioAnalysis ali audioFlux\n",
    "import numpy as np\n",
    "import os\n",
    "import h5py\n",
    "import time\n",
    "import datetime\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "# Preprocessing, Metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Keras, Classification\n",
    "import keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from sklearn.svm import SVC\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.utils import to_categorical\n",
    "# Parameters\n",
    "genres = np.array('pop rock classical blues country disco metal jazz reggae hiphop'.sp\n",
    "n_genres = len(genres)\n",
    "n_genres_files = 100\n",
    "# n_genres_files = 1\n",
    "# size of feature vector extracted by model for each segment of audio\n",
    "embedding_size = 512\n",
    "# number of time windows that model outputs for each segment of audio\n",
    "# 5 sec segment - 46 time windows\n",
    "n_windows = 46\n",
    "# number of 5 sec segments per audio file\n",
    "n_parts_sig = 6\n",
    "data_dir = './archive/Data/genres_original/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_chunks = n_genres * n_genres_files * n_parts_sig\n",
    "# stores features extracted from each segment of audio;\n",
    "# parameters: [NumberOfSignalParts,NumberOfWindows,NumberOfFeatures]\n",
    "data = np.zeros((total_chunks, n_windows, embedding_size))\n",
    "# stores genre labels corresponding to each feature in data\n",
    "# parameters: [NumberOfSignalParts,1]\n",
    "data_labels = np.zeros((total_chunks, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load and preprocess data\n",
    "# Dataset\n",
    "data_index = 0 # keep track where to store emmbeddings\n",
    "for i_genre, genre in enumerate(genres):\n",
    "    genre_path = os.path.join(data_dir, genre)\n",
    "    files = os.listdir(genre_path)\n",
    "    for file in files:\n",
    "        fn = os.path.join(genre_path, file)\n",
    "        try:\n",
    "            # Load file (sig-signal; sr-sampling rate)\n",
    "            # Load full 30-sec audio file\n",
    "            sig, sr = librosa.load(fn, mono=True, duration=30)\n",
    "            # Divide the 30-sec file into 6 chunks\n",
    "            for start in range(0, len(sig), sr*5):\n",
    "            segment = sig[start : start+sr*5]\n",
    "            if len(segment) < sr*5:\n",
    "            continue\n",
    "            emb, _ = openl3.get_audio_embedding(segment, sr, content_type=\"music\",\n",
    "            if data_index < len(data):\n",
    "            # Features - Data\n",
    "            data[data_index, :, :] = emb\n",
    "            # Genre - Label\n",
    "            data_labels[data_index] = i_genre\n",
    "            data_index = data_index + 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {fn}: {e}\")\n",
    "            pass\n",
    "\n",
    "# Save to h5 file\n",
    "hf = h5py.File('dataset_openl3.h5', 'w')\n",
    "hf.create_dataset('data', data=data)\n",
    "hf.create_dataset('data_labels', data=data_labels)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "scaler = StandardScaler() # works with 2D array\n",
    "# reshaping into 2D array where each row is a time window of an embedding\n",
    "x = np.reshape(data, newshape=(data.shape[0]*data.shape[1], data.shape[2]))\n",
    "# after scaling, data is reshaped into its original 3D shape\n",
    "X = np.reshape(scaler.fit_transform(np.array(x, dtype = float)), newshape=data.shape)\n",
    "# Split into test and train\n",
    "# Why stratify=data_labels?\n",
    "# stratify ensures that the labels in train and test set is the same as in the orignal\n",
    "# Check the histograms, try removing stratify\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, data_labels, test_size=0.2, str\n",
    "# Split into train and valid\n",
    "# Why stratify=y_train?\n",
    "# Check the histograms, try removing stratify\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, st\n",
    "# Sizes\n",
    "print('Original dims')\n",
    "print('Train X:', np.shape(X_train))\n",
    "print('Train Y:', np.shape(y_train))\n",
    "print('Test X:', np.shape(X_test))\n",
    "print('Test Y:', np.shape(y_test))\n",
    "print('Val X:', np.shape(X_val))\n",
    "print('Val Y:', np.shape(y_val))\n",
    "# Why correction?\n",
    "# probably for compatibility with some Keras layers\n",
    "print('Corrected dims')\n",
    "X_train = np.expand_dims(X_train, 3)\n",
    "X_test = np.expand_dims(X_test, 3)\n",
    "X_val = np.expand_dims(X_val, 3)\n",
    "print('Train X:', np.shape(X_train))\n",
    "print('Train Y:', np.shape(y_train))\n",
    "print('Test X:', np.shape(X_test))\n",
    "print('Test Y:', np.shape(y_test))\n",
    "print('Val X:', np.shape(X_val))\n",
    "print('Val Y:', np.shape(y_val))\n",
    "plt.hist(y_train, bins=n_genres, rwidth=0.7)\n",
    "plt.show()\n",
    "plt.hist(y_test, bins=n_genres, rwidth=0.7)\n",
    "plt.show()\n",
    "plt.hist(y_val, bins=n_genres, rwidth=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(np.array(data, dtype = float))\n",
    "\n",
    "# Split into test and train\n",
    "# Why stratify=data_labels?\n",
    "# Check the histograms, try removing stratify\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, data_labels, test_size=0.2, stratify=data_labels)\n",
    "\n",
    "# Split into train and valid\n",
    "# Why stratify=y_train?\n",
    "# Check the histograms, try removing stratify\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, stratify=y_train)\n",
    "\n",
    "# Sizes\n",
    "print('Train:', np.shape(y_train))\n",
    "print('Test:', np.shape(y_test))\n",
    "print('Val:', np.shape(y_val))\n",
    "\n",
    "# The truth is â€” there is no optimal split percentage\n",
    "# train 80%; valid 10%; test 10%\n",
    "# train 70%; valid 15%; test 15%\n",
    "# tarin 60%; valid 20%; test 20%\n",
    "\n",
    "plt.hist(y_train, bins=n_genres, rwidth=0.7)\n",
    "plt.show()\n",
    "plt.hist(y_test, bins=n_genres, rwidth=0.7)\n",
    "plt.show()\n",
    "plt.hist(y_val, bins=n_genres, rwidth=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32, activation='relu', input_shape=(X_train.shape[1],))) # Input layer - number of features\n",
    "# Fix the model - add extra layers, change the number of neurons, etc...\n",
    "model.add(layers.Dense(n_genres, activation='softmax')) # Output layer - 10 genres\n",
    "\n",
    "opt = keras.optimizers.Adam(learning_rate = 0.001) # Maybe a bit too high?\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy() # Computes the crossentropy loss between the labels and predictions\n",
    "metr = keras.metrics.SparseCategoricalAccuracy() # Calculates how often predictions match integer labels\n",
    "model.compile(optimizer=opt, loss=loss, metrics=metr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopping criterion to avoid overfitting\n",
    "# patience: Number of epochs with no improvement after which training will be stopped.\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# Save best weights\n",
    "model_checkpoint = ModelCheckpoint(\"weights.h5\", save_best_only=True, save_weights_only=True)\n",
    "\n",
    "# Train\n",
    "t_epochs = 50 # Needs to be tuned\n",
    "b_size = 8 # Needs to be tuned as well - What is batch_size?\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=t_epochs, batch_size=b_size,\n",
    "                    callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "# Load best weights\n",
    "model.load_weights(\"weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets observe the loss metric on both the training (blue) and validation (orange) set\n",
    "# What do we noice?\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['sparse_categorical_accuracy'])\n",
    "plt.plot(history.history['val_sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to evaluate our model on train and test data\n",
    "\n",
    "# Train NN\n",
    "test_loss, test_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "print('Acc train NN: %.3f' % test_acc)\n",
    "\n",
    "# Test NN\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Acc test NN: %.3f' % test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test NN\n",
    "# Predictions for additional analysis\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Confusion matrix\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "conf = confusion_matrix(y_test, predicted_labels, normalize=\"pred\") # Normalize pred! Explain why?\n",
    "\n",
    "# Visualise confusion matrix\n",
    "plt.imshow(conf)\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.yticks(np.arange(n_genres), genres)\n",
    "plt.xticks(np.arange(n_genres), genres, rotation='vertical')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Internet radio\n",
    "import miniaudio\n",
    "from IPython.display import clear_output, display\n",
    "current_title = \"\"\n",
    "\n",
    "def title(client: miniaudio.IceCastClient, title: str):\n",
    "    global current_title \n",
    "    current_title = title\n",
    "\n",
    "def stream_processing(source):\n",
    "    while True:\n",
    "        # Get frame - Only one channel - The chunk of signal is too small!\n",
    "        sample_data = np.array(source.send(8192))[0::2]\n",
    "\n",
    "        # Features\n",
    "        feat = extract_features(sample_data, 44100, n_features, n_mfcc_coef)\n",
    "\n",
    "        # Normalization\n",
    "        feat_norm = scaler.transform(feat.reshape(1, -1))\n",
    "\n",
    "        # Guess\n",
    "        pred_nn = model.predict(feat_norm, verbose=0)\n",
    "\n",
    "        # Output\n",
    "        clear_output(wait=True)\n",
    "        print(\"Title: \" + current_title)\n",
    "        print(datetime.datetime.now())\n",
    "        print(\"NN: \" + genres[np.argmax(pred_nn[0])])\n",
    "\n",
    "        yield sample_data\n",
    "\n",
    "# Internet radio source - Radio 1\n",
    "source = miniaudio.IceCastClient(\"http://live1.radio1.si/Radio1\", update_stream_title=title)\n",
    "\n",
    "print(\"Connected\")\n",
    "print(\"Station: \", source.station_name)\n",
    "\n",
    "# Stream\n",
    "stream_in = miniaudio.stream_any(source, source.audio_format, output_format=miniaudio.SampleFormat.FLOAT32)\n",
    "\n",
    "# Device\n",
    "device = miniaudio.PlaybackDevice(output_format=miniaudio.SampleFormat.FLOAT32, nchannels=1, sample_rate=44100)\n",
    "\n",
    "stream = stream_processing(stream_in)\n",
    "next(stream)\n",
    "device.start(stream)\n",
    "\n",
    "while True:\n",
    "    time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import miniaudio\n",
    "\n",
    "stream = miniaudio.stream_any(\"samples/music.mp3\")\n",
    "with miniaudio.PlaybackDevice() as device:\n",
    "    device.start(stream)\n",
    "    input(\"Audio file playing in the background. Enter to stop playback: \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
