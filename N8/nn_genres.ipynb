{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base\n",
    "import librosa # alternativa pyAudioAnalysis ali audioFlux\n",
    "import numpy as np\n",
    "import os\n",
    "import h5py\n",
    "import time\n",
    "import datetime\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Preprocessing, Metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Keras, Classification\n",
    "import keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from sklearn.svm import SVC\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Parameters\n",
    "genres = np.array('pop rock classical blues country disco metal jazz reggae hiphop'.split())\n",
    "n_genres = len(genres)\n",
    "n_genres_files = 100\n",
    "n_features = 2 # Without MFCC - Change!\n",
    "n_mfcc_coef = 10 # Experiment with no. of coeffs.\n",
    "n_parts_sig = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(y, sr, n_features, n_mfcc_coef, n_fft=512, hop_length=160, window=signal.windows.hamming(512), fmin=300, fmax=8000):\n",
    "    vect = np.zeros(n_features + n_mfcc_coef)\n",
    "\n",
    "    vect[0] = np.mean(librosa.feature.zero_crossing_rate(y))\n",
    "    vect[1] = np.mean(librosa.feature.tempo(y=y, sr=sr))\n",
    "    # Add more feature extractors!\n",
    "\n",
    "    # MFCC\n",
    "    # Can use Kapre (https://github.com/keunwoochoi/kapre) GPU\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc_coef)\n",
    "\n",
    "    for i in range(0, n_mfcc_coef):\n",
    "        vect[i + n_features] = np.mean(mfcc[i])\n",
    "\n",
    "    return vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset - Will take some time to generate\n",
    "data = np.zeros((n_genres * n_genres_files * n_parts_sig, n_features + n_mfcc_coef))\n",
    "data_labels = np.zeros((n_genres * n_genres_files * n_parts_sig, 1))\n",
    "\n",
    "data_index = 0\n",
    "for i_genre in range(0, n_genres):\n",
    "    for filename in os.listdir(f'./genres/{genres[i_genre]}'):\n",
    "        fn = f'./genres/{genres[i_genre]}/{filename}'\n",
    "        \n",
    "        # There is one problematic file - format problem (can try ffmpeg decoder)\n",
    "        try:\n",
    "            # Load file (sig-signal; sr-sampling rate)\n",
    "            sig, sr = librosa.load(fn, mono=True, duration=30)\n",
    "\n",
    "            # Split signals into smaller chunks\n",
    "            for y in np.split(sig, n_parts_sig): # Not only into two - more!\n",
    "                # Features - Data\n",
    "                data[data_index, 0:n_features + n_mfcc_coef] = extract_features(sig, sr, n_features, n_mfcc_coef)\n",
    "\n",
    "                # Genre - Label\n",
    "                data_labels[data_index] = i_genre\n",
    "\n",
    "                data_index = data_index + 1\n",
    "        except:\n",
    "            pass       \n",
    "\n",
    "# Save to h5 file\n",
    "hf = h5py.File('dataset.h5', 'w')\n",
    "hf.create_dataset('data', data=data)\n",
    "hf.create_dataset('data_labels', data=data_labels)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from h5 file\n",
    "hf = h5py.File('dataset.h5', 'r')\n",
    "\n",
    "data = hf.get('data')\n",
    "data = np.array(data)\n",
    "\n",
    "data_labels = hf.get('data_labels')\n",
    "data_labels = np.array(data_labels)\n",
    "\n",
    "print('Data size:', np.shape(data))\n",
    "print('Data_labels size:', np.shape(data_labels))\n",
    "\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(np.array(data, dtype = float))\n",
    "\n",
    "# Split into test and train\n",
    "# Why stratify=data_labels?\n",
    "# Check the histograms, try removing stratify\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, data_labels, test_size=0.2, stratify=data_labels)\n",
    "\n",
    "# Split into train and valid\n",
    "# Why stratify=y_train?\n",
    "# Check the histograms, try removing stratify\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, stratify=y_train)\n",
    "\n",
    "# Sizes\n",
    "print('Train:', np.shape(y_train))\n",
    "print('Test:', np.shape(y_test))\n",
    "print('Val:', np.shape(y_val))\n",
    "\n",
    "# The truth is â€” there is no optimal split percentage\n",
    "# train 80%; valid 10%; test 10%\n",
    "# train 70%; valid 15%; test 15%\n",
    "# tarin 60%; valid 20%; test 20%\n",
    "\n",
    "plt.hist(y_train, bins=n_genres, rwidth=0.7)\n",
    "plt.show()\n",
    "plt.hist(y_test, bins=n_genres, rwidth=0.7)\n",
    "plt.show()\n",
    "plt.hist(y_val, bins=n_genres, rwidth=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32, activation='relu', input_shape=(X_train.shape[1],))) # Input layer - number of features\n",
    "# Fix the model - add extra layers, change the number of neurons, etc...\n",
    "model.add(layers.Dense(n_genres, activation='softmax')) # Output layer - 10 genres\n",
    "\n",
    "opt = keras.optimizers.Adam(learning_rate = 0.001) # Maybe a bit too high?\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy() # Computes the crossentropy loss between the labels and predictions\n",
    "metr = keras.metrics.SparseCategoricalAccuracy() # Calculates how often predictions match integer labels\n",
    "model.compile(optimizer=opt, loss=loss, metrics=metr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopping criterion to avoid overfitting\n",
    "# patience: Number of epochs with no improvement after which training will be stopped.\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# Save best weights\n",
    "model_checkpoint = ModelCheckpoint(\"weights.h5\", save_best_only=True, save_weights_only=True)\n",
    "\n",
    "# Train\n",
    "t_epochs = 50 # Needs to be tuned\n",
    "b_size = 8 # Needs to be tuned as well - What is batch_size?\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=t_epochs, batch_size=b_size,\n",
    "                    callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "# Load best weights\n",
    "model.load_weights(\"weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets observe the loss metric on both the training (blue) and validation (orange) set\n",
    "# What do we noice?\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['sparse_categorical_accuracy'])\n",
    "plt.plot(history.history['val_sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to evaluate our model on train and test data\n",
    "\n",
    "# Train NN\n",
    "test_loss, test_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "print('Acc train NN: %.3f' % test_acc)\n",
    "\n",
    "# Test NN\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Acc test NN: %.3f' % test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test NN\n",
    "# Predictions for additional analysis\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Confusion matrix\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "conf = confusion_matrix(y_test, predicted_labels, normalize=\"pred\") # Normalize pred! Explain why?\n",
    "\n",
    "# Visualise confusion matrix\n",
    "plt.imshow(conf)\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.yticks(np.arange(n_genres), genres)\n",
    "plt.xticks(np.arange(n_genres), genres, rotation='vertical')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Internet radio\n",
    "import miniaudio\n",
    "from IPython.display import clear_output, display\n",
    "current_title = \"\"\n",
    "\n",
    "def title(client: miniaudio.IceCastClient, title: str):\n",
    "    global current_title \n",
    "    current_title = title\n",
    "\n",
    "def stream_processing(source):\n",
    "    while True:\n",
    "        # Get frame - Only one channel - The chunk of signal is too small!\n",
    "        sample_data = np.array(source.send(8192))[0::2]\n",
    "\n",
    "        # Features\n",
    "        feat = extract_features(sample_data, 44100, n_features, n_mfcc_coef)\n",
    "\n",
    "        # Normalization\n",
    "        feat_norm = scaler.transform(feat.reshape(1, -1))\n",
    "\n",
    "        # Guess\n",
    "        pred_nn = model.predict(feat_norm, verbose=0)\n",
    "\n",
    "        # Output\n",
    "        clear_output(wait=True)\n",
    "        print(\"Title: \" + current_title)\n",
    "        print(datetime.datetime.now())\n",
    "        print(\"NN: \" + genres[np.argmax(pred_nn[0])])\n",
    "\n",
    "        yield sample_data\n",
    "\n",
    "# Internet radio source - Radio 1\n",
    "source = miniaudio.IceCastClient(\"http://live1.radio1.si/Radio1\", update_stream_title=title)\n",
    "\n",
    "print(\"Connected\")\n",
    "print(\"Station: \", source.station_name)\n",
    "\n",
    "# Stream\n",
    "stream_in = miniaudio.stream_any(source, source.audio_format, output_format=miniaudio.SampleFormat.FLOAT32)\n",
    "\n",
    "# Device\n",
    "device = miniaudio.PlaybackDevice(output_format=miniaudio.SampleFormat.FLOAT32, nchannels=1, sample_rate=44100)\n",
    "\n",
    "stream = stream_processing(stream_in)\n",
    "next(stream)\n",
    "device.start(stream)\n",
    "\n",
    "while True:\n",
    "    time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import miniaudio\n",
    "\n",
    "stream = miniaudio.stream_any(\"samples/music.mp3\")\n",
    "with miniaudio.PlaybackDevice() as device:\n",
    "    device.start(stream)\n",
    "    input(\"Audio file playing in the background. Enter to stop playback: \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
